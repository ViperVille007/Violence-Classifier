{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import string \n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('positive_train_new.txt','r',encoding = 'utf8') as inFile, open('Filtered_positive_train_new.txt','w',encoding = 'utf8') as outFile: \n",
    "    for line in inFile.readlines(): \n",
    "        print(\" \".join([word for word in line.lower().translate(str.maketrans('', '', string.punctuation)).split()  \n",
    "            if len(word) >=4 and word not in stopwords.words('english')]), file=outFile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('negative_train_new.txt','r',encoding = 'utf8') as inFile, open('Filtered_negative_train_new.txt','w',encoding = 'utf8') as outFile: \n",
    "    for line in inFile.readlines(): \n",
    "        print(\" \".join([word for word in line.lower().translate(str.maketrans('', '', string.punctuation)).split()  \n",
    "            if len(word) >=4 and word not in stopwords.words('english')]), file=outFile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the other necessary libraries first\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the dataset\n",
    "#There are two separate text files, containing the 2 classes of documents\n",
    "data = open('Filtered_positive_train_new.txt',encoding=\"utf8\").read()\n",
    "data1 = open('Filtered_negative_train_new.txt',encoding=\"utf8\").read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is no separate column denoting the label of each document.\n",
    "#So adding label to each (Label_1 denotes violent; label_2 denotes violent)\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(0)\n",
    "#Joining elements from both the classes together into one single file \n",
    "    texts.append(\" \".join(content[1:]))\n",
    "for i, line in enumerate(data1.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(1)\n",
    "    texts.append(\" \".join(content[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe using above texts and labels\n",
    "import pandas\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>husband held charges cheating intimidation acc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3204</th>\n",
       "      <td>vicechairman managing director infrastructure ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>custody held charge isinspired group members d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>departed souls traditional manners home attent...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>lynched manipurs imphal suspicion vehicle thef...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>clash ramangaram followed stoning religious ka...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>normalthe times india 1861current 1992 proques...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2411</th>\n",
       "      <td>drains covered 2019 0602 drainage lines perman...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3583</th>\n",
       "      <td>held killing husband teenage daughter also inv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4059</th>\n",
       "      <td>march srinivas producer kannada film ranam who...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4968 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "3309  husband held charges cheating intimidation acc...      1\n",
       "3204  vicechairman managing director infrastructure ...      1\n",
       "3420  custody held charge isinspired group members d...      1\n",
       "3979  departed souls traditional manners home attent...      1\n",
       "1154  lynched manipurs imphal suspicion vehicle thef...      0\n",
       "...                                                 ...    ...\n",
       "962   clash ramangaram followed stoning religious ka...      0\n",
       "812   normalthe times india 1861current 1992 proques...      0\n",
       "2411  drains covered 2019 0602 drainage lines perman...      1\n",
       "3583  held killing husband teenage daughter also inv...      1\n",
       "4059  march srinivas producer kannada film ranam who...      1\n",
       "\n",
       "[4968 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, all instances of one class are clumped together\n",
    "#Shuffling the rows of the dataframe to avoid discrepancies in future\n",
    "from sklearn.utils import shuffle\n",
    "trainDF = shuffle(trainDF)\n",
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Times_January.txt',encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tripathy to debut in Sandalwood () Pitobash Tr...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>isn’t respect or fanfare: Yash on the fan who ...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>on ‘noisy’ bikes beat up 2 policemen | Jan 12,...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ali Khan soaking in the sun will make you beam...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rescues stranded tourists from north Sikkim in...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13854</th>\n",
       "      <td>that propelled BJP to power in 2014 has dissip...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13855</th>\n",
       "      <td>Delhi: Accused Mehtab arrested day after he ki...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13856</th>\n",
       "      <td>Nath may have been framed in rape case, observ...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13857</th>\n",
       "      <td>‘Can’t remove CM over pending cases’ | Jan 9, ...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13858</th>\n",
       "      <td></td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13859 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text label\n",
       "0      Tripathy to debut in Sandalwood () Pitobash Tr...    XX\n",
       "1      isn’t respect or fanfare: Yash on the fan who ...    XX\n",
       "2      on ‘noisy’ bikes beat up 2 policemen | Jan 12,...    XX\n",
       "3      Ali Khan soaking in the sun will make you beam...    XX\n",
       "4      rescues stranded tourists from north Sikkim in...    XX\n",
       "...                                                  ...   ...\n",
       "13854  that propelled BJP to power in 2014 has dissip...    XX\n",
       "13855  Delhi: Accused Mehtab arrested day after he ki...    XX\n",
       "13856  Nath may have been framed in rape case, observ...    XX\n",
       "13857  ‘Can’t remove CM over pending cases’ | Jan 9, ...    XX\n",
       "13858                                                       XX\n",
       "\n",
       "[13859 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(\"XX\") \n",
    "    texts.append(\" \".join(content[1:]))\n",
    "    \n",
    "#Creating a dataframe using above texts and labels\n",
    "import pandas\n",
    "testDF = pandas.DataFrame()\n",
    "testDF['text'] = texts\n",
    "testDF['label'] = labels\n",
    "testDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>husband held charges cheating intimidation acc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3204</th>\n",
       "      <td>vicechairman managing director infrastructure ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>custody held charge isinspired group members d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>departed souls traditional manners home attent...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>lynched manipurs imphal suspicion vehicle thef...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13776</th>\n",
       "      <td>protest in Kochi: Writing poems is ‘nun’ of he...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088</th>\n",
       "      <td>Man posts private chats with ex on porn site, ...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>I profited from paintings &amp; I’ll quit: Mamata ...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7050</th>\n",
       "      <td>Patel gets the temperature soaring with her la...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>BSNL Bengaluru and the Union Bank of India hav...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5968 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text label\n",
       "3309   husband held charges cheating intimidation acc...     1\n",
       "3204   vicechairman managing director infrastructure ...     1\n",
       "3420   custody held charge isinspired group members d...     1\n",
       "3979   departed souls traditional manners home attent...     1\n",
       "1154   lynched manipurs imphal suspicion vehicle thef...     0\n",
       "...                                                  ...   ...\n",
       "13776  protest in Kochi: Writing poems is ‘nun’ of he...    XX\n",
       "7088   Man posts private chats with ex on porn site, ...    XX\n",
       "10501  I profited from paintings & I’ll quit: Mamata ...    XX\n",
       "7050   Patel gets the temperature soaring with her la...    XX\n",
       "1488   BSNL Bengaluru and the Union Bank of India hav...    XX\n",
       "\n",
       "[5968 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF = shuffle(testDF)\n",
    "\n",
    "testDF1= testDF.iloc[0:1000]\n",
    "result = trainDF.append(testDF1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = result\n",
    "#Splitting the dataset into training(80%) and validation(20%) sets  \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, train_y=trainDF['text'].iloc[0:4968], trainDF['label'].iloc[0:4968]\n",
    "valid_x, valid_y=trainDF['text'].iloc[4968:], trainDF['label'].iloc[4968:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3309    1\n",
       "3204    1\n",
       "3420    1\n",
       "3979    1\n",
       "1154    0\n",
       "       ..\n",
       "962     0\n",
       "812     0\n",
       "2411    1\n",
       "3583    1\n",
       "4059    1\n",
       "Name: label, Length: 4968, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8632     XX\n",
       "5828     XX\n",
       "8912     XX\n",
       "11321    XX\n",
       "13161    XX\n",
       "         ..\n",
       "13776    XX\n",
       "7088     XX\n",
       "10501    XX\n",
       "7050     XX\n",
       "1488     XX\n",
       "Name: label, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding the class-labels or the target variable \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step is the feature engineering step. \n",
    "#Raw text data will be transformed into feature vectors and new features will be created using the existing dataset.\n",
    "\n",
    "#count vectorizer, tf-idf, word embeddings, topic models, part-of-speech tagging,\n",
    "\n",
    "#Creating count vectorizer object and using it to transform data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANKAN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:501: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=3000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a pre-trained word embedding to create a mapping of token and their respective embeddings\n",
    "import numpy\n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('C:/Users/ANKAN/Downloads/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec',encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "\n",
    "#Creatng a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "#Converting text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=50)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering - introducing new features that are expected to be useful\n",
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ANKAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6987</th>\n",
       "      <td>roundabouts: Committee to check pricing of mat...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1670</td>\n",
       "      <td>266</td>\n",
       "      <td>6.254682</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>SC defers hearing PIL against bill | Jan 15, 2...</td>\n",
       "      <td>XX</td>\n",
       "      <td>876</td>\n",
       "      <td>151</td>\n",
       "      <td>5.763158</td>\n",
       "      <td>32</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7472</th>\n",
       "      <td>writer and professor of English Germaine Greer...</td>\n",
       "      <td>XX</td>\n",
       "      <td>2095</td>\n",
       "      <td>371</td>\n",
       "      <td>5.631720</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>10ha land needs to be acquired for Metro Phase...</td>\n",
       "      <td>XX</td>\n",
       "      <td>3709</td>\n",
       "      <td>639</td>\n",
       "      <td>5.795312</td>\n",
       "      <td>105</td>\n",
       "      <td>92</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441</th>\n",
       "      <td>Postcard campaign urges govt to back BMTC | Ja...</td>\n",
       "      <td>XX</td>\n",
       "      <td>2004</td>\n",
       "      <td>321</td>\n",
       "      <td>6.223602</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13776</th>\n",
       "      <td>protest in Kochi: Writing poems is ‘nun’ of he...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1981</td>\n",
       "      <td>350</td>\n",
       "      <td>5.643875</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088</th>\n",
       "      <td>Man posts private chats with ex on porn site, ...</td>\n",
       "      <td>XX</td>\n",
       "      <td>710</td>\n",
       "      <td>119</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>I profited from paintings &amp; I’ll quit: Mamata ...</td>\n",
       "      <td>XX</td>\n",
       "      <td>897</td>\n",
       "      <td>151</td>\n",
       "      <td>5.901316</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7050</th>\n",
       "      <td>Patel gets the temperature soaring with her la...</td>\n",
       "      <td>XX</td>\n",
       "      <td>738</td>\n",
       "      <td>123</td>\n",
       "      <td>5.951613</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>BSNL Bengaluru and the Union Bank of India hav...</td>\n",
       "      <td>XX</td>\n",
       "      <td>3086</td>\n",
       "      <td>513</td>\n",
       "      <td>6.003891</td>\n",
       "      <td>49</td>\n",
       "      <td>62</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text label  char_count  \\\n",
       "6987   roundabouts: Committee to check pricing of mat...    XX        1670   \n",
       "2254   SC defers hearing PIL against bill | Jan 15, 2...    XX         876   \n",
       "7472   writer and professor of English Germaine Greer...    XX        2095   \n",
       "155    10ha land needs to be acquired for Metro Phase...    XX        3709   \n",
       "3441   Postcard campaign urges govt to back BMTC | Ja...    XX        2004   \n",
       "13776  protest in Kochi: Writing poems is ‘nun’ of he...    XX        1981   \n",
       "7088   Man posts private chats with ex on porn site, ...    XX         710   \n",
       "10501  I profited from paintings & I’ll quit: Mamata ...    XX         897   \n",
       "7050   Patel gets the temperature soaring with her la...    XX         738   \n",
       "1488   BSNL Bengaluru and the Union Bank of India hav...    XX        3086   \n",
       "\n",
       "       word_count  word_density  punctuation_count  title_word_count  \\\n",
       "6987          266      6.254682                 24                24   \n",
       "2254          151      5.763158                 32                31   \n",
       "7472          371      5.631720                 57                44   \n",
       "155           639      5.795312                105                92   \n",
       "3441          321      6.223602                 45                37   \n",
       "13776         350      5.643875                 38                48   \n",
       "7088          119      5.916667                 23                13   \n",
       "10501         151      5.901316                 21                27   \n",
       "7050          123      5.951613                 18                11   \n",
       "1488          513      6.003891                 49                62   \n",
       "\n",
       "       upper_case_word_count  \n",
       "6987                       1  \n",
       "2254                      11  \n",
       "7472                       5  \n",
       "155                       20  \n",
       "3441                      11  \n",
       "13776                      8  \n",
       "7088                       2  \n",
       "10501                      6  \n",
       "7050                       0  \n",
       "1488                      14  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Count Vectors:  0.789\n",
      "10 fold Cross-validated score:  0.9808776043356916 \n",
      "\n",
      "Naive Bayes WordLevel TF-IDF:  0.641\n",
      "10 fold Cross-validated score:  0.9840997598494191 \n",
      "\n",
      "Naive Bayes N-Gram Vectors:  1.0\n",
      "10 fold Cross-validated score:  0.8780172973323814 \n",
      "\n",
      "Naive Bayes CharLevel Vectors:  0.892\n",
      "10 fold Cross-validated score:  0.9750413772960342 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"Naive Bayes Count Vectors: \", accuracy)\n",
    "scores = cross_val_score(naive_bayes.MultinomialNB(), xtrain_count, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Naive Bayes WordLevel TF-IDF: \", accuracy)\n",
    "scores = cross_val_score(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"Naive Bayes N-Gram Vectors: \", accuracy)\n",
    "scores = cross_val_score(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"Naive Bayes CharLevel Vectors: \", accuracy)\n",
    "scores = cross_val_score(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, Count Vectors:  0.001\n",
      "10 fold Cross-validated score:  0.9883255176218603 \n",
      "\n",
      "Logistic Regression, WordLevel TF-IDF:  0.008\n",
      "10 fold Cross-validated score:  0.9913456545725969 \n",
      "\n",
      "Logistic Regression, N-Gram Vectors:  0.306\n",
      "10 fold Cross-validated score:  0.9553141429220483 \n",
      "\n",
      "Logistic Regression, CharLevel Vectors:  0.108\n",
      "10 fold Cross-validated score:  0.9800731810216137 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(max_iter=4000), xtrain_count, train_y, xvalid_count)\n",
    "print (\"Logistic Regression, Count Vectors: \", accuracy)\n",
    "scores = cross_val_score(linear_model.LogisticRegression(max_iter=4000), xtrain_count, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Logistic Regression, WordLevel TF-IDF: \", accuracy)\n",
    "scores = cross_val_score(linear_model.LogisticRegression(max_iter=4000), xtrain_tfidf, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"Logistic Regression, N-Gram Vectors: \", accuracy)\n",
    "scores = cross_val_score(linear_model.LogisticRegression(max_iter=4000), xtrain_tfidf_ngram, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"Logistic Regression, CharLevel Vectors: \", accuracy)\n",
    "scores = cross_val_score(linear_model.LogisticRegression(max_iter=4000), xtrain_tfidf_ngram_chars, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.049\n",
      "10 fold Cross-validated score:  0.9567242162653338 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)\n",
    "scores = cross_val_score(svm.SVC(), xtrain_tfidf_ngram, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests, Count Vectors:  0.014\n",
      "10 fold Cross-validated score:  0.9885283475043811 \n",
      "\n",
      "Random Forests, WordLevel TF-IDF:  0.002\n",
      "10 fold Cross-validated score:  0.988527536184851 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forests on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"Random Forests, Count Vectors: \", accuracy)\n",
    "scores = cross_val_score(ensemble.RandomForestClassifier(), xtrain_count, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Random Forests, WordLevel TF-IDF: \", accuracy)\n",
    "scores = cross_val_score(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.015\n",
      "10 fold Cross-validated score:  0.9861114266242618 \n",
      "\n",
      "Xgb, WordLevel TF-IDF:  0.006\n",
      "10 fold Cross-validated score:  0.9853074089699488 \n",
      "\n",
      "Xgb, CharLevel Vectors:  0.006\n",
      "10 fold Cross-validated score:  0.9855098331927046 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "scores = cross_val_score(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "scores = cross_val_score(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)\n",
    "scores = cross_val_score(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(xtrain_tfidf_ngram_chars, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=lr.predict(xvalid_tfidf_ngram_chars)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-69-dc9592408b33>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testDF1['predicted']=y_pred\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8632</th>\n",
       "      <td>suspect held in Sahar firing case Mumbai: The ...</td>\n",
       "      <td>XX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10057</th>\n",
       "      <td>farmers killed over land dispute PATNA: Two fa...</td>\n",
       "      <td>XX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>fencing poses threat to black bucks in Haryana...</td>\n",
       "      <td>XX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6375</th>\n",
       "      <td>Poll rigging claim: Neta wants inquiry | Jan 2...</td>\n",
       "      <td>XX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12080</th>\n",
       "      <td>boy crushed to death in Halol | Jan 6, 2019, 0...</td>\n",
       "      <td>XX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>for gaving baby child, woman ends life | Jan 1...</td>\n",
       "      <td>XX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5188</th>\n",
       "      <td>refuses to marry drunk cop in Bhagalpur BHAGAL...</td>\n",
       "      <td>XX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11823</th>\n",
       "      <td>‘The Notebook’ to be made into Broadway musica...</td>\n",
       "      <td>XX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8314</th>\n",
       "      <td>() Television actress Kavita Kaushik and husba...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7950</th>\n",
       "      <td>Kapoor and Khushi Kapoor's candid pictures Jan...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text label  predicted\n",
       "8632   suspect held in Sahar firing case Mumbai: The ...    XX          0\n",
       "10057  farmers killed over land dispute PATNA: Two fa...    XX          0\n",
       "1543   fencing poses threat to black bucks in Haryana...    XX          0\n",
       "6375   Poll rigging claim: Neta wants inquiry | Jan 2...    XX          0\n",
       "12080  boy crushed to death in Halol | Jan 6, 2019, 0...    XX          0\n",
       "...                                                  ...   ...        ...\n",
       "2948   for gaving baby child, woman ends life | Jan 1...    XX          0\n",
       "5188   refuses to marry drunk cop in Bhagalpur BHAGAL...    XX          0\n",
       "11823  ‘The Notebook’ to be made into Broadway musica...    XX          0\n",
       "8314   () Television actress Kavita Kaushik and husba...    XX          1\n",
       "7950   Kapoor and Khushi Kapoor's candid pictures Jan...    XX          1\n",
       "\n",
       "[110 rows x 3 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF1['predicted']=y_pred\n",
    "df=testDF1.sort_values('predicted')\n",
    "df.head(110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('stage_2_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
