{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import string \n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('positive_train_new.txt','r',encoding = 'utf8') as inFile, open('Filtered_positive_train_new.txt','w',encoding = 'utf8') as outFile: \n",
    "    for line in inFile.readlines(): \n",
    "        print(\" \".join([word for word in line.lower().translate(str.maketrans('', '', string.punctuation)).split()  \n",
    "            if len(word) >=4 and word not in stopwords.words('english')]), file=outFile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('not_violent_train.txt','r',encoding = 'utf8') as inFile, open('Filtered_not_violent.txt','w',encoding = 'utf8') as outFile: \n",
    "    for line in inFile.readlines(): \n",
    "        print(\" \".join([word for word in line.lower().translate(str.maketrans('', '', string.punctuation)).split()  \n",
    "            if len(word) >=4 and word not in stopwords.words('english')]), file=outFile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the other necessary libraries first\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the dataset\n",
    "#There are two separate text files, containing the 2 classes of documents\n",
    "data = open('Filtered_positive_train_new.txt',encoding=\"utf8\").read()\n",
    "data1 = open('Filtered_not_violent.txt',encoding=\"utf8\").read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is no separate column denoting the label of each document.\n",
    "#So adding label to each (Label_1 denotes violent; label_2 denotes violent)\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(\"label_1\")\n",
    "#Joining elements from both the classes together into one single file \n",
    "    texts.append(\" \".join(content[1:]))\n",
    "for i, line in enumerate(data1.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(\"label_2\")\n",
    "    texts.append(\" \".join(content[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe using above texts and labels\n",
    "import pandas\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>sacrilege therefore desecration unknown also s...</td>\n",
       "      <td>label_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>order receipt sent prime minister narendra mod...</td>\n",
       "      <td>label_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>demandedthe times india j86jcurrent 1956 proqu...</td>\n",
       "      <td>label_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>four monkeys found dead different parts udupi ...</td>\n",
       "      <td>label_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>celebration christian family attacked hindu ra...</td>\n",
       "      <td>label_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4226</th>\n",
       "      <td>father suffers heart attack incident took plac...</td>\n",
       "      <td>label_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>driver mumbai allegedly physically assaulted g...</td>\n",
       "      <td>label_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>veteran industrialist rahul bajaj city attend ...</td>\n",
       "      <td>label_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>victim’s father arrested charge covering tortu...</td>\n",
       "      <td>label_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>hide fight secure immediately” university said...</td>\n",
       "      <td>label_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4992 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    label\n",
       "414   sacrilege therefore desecration unknown also s...  label_1\n",
       "2038  order receipt sent prime minister narendra mod...  label_2\n",
       "605   demandedthe times india j86jcurrent 1956 proqu...  label_1\n",
       "4240  four monkeys found dead different parts udupi ...  label_2\n",
       "1804  celebration christian family attacked hindu ra...  label_1\n",
       "...                                                 ...      ...\n",
       "4226  father suffers heart attack incident took plac...  label_2\n",
       "237   driver mumbai allegedly physically assaulted g...  label_1\n",
       "2109  veteran industrialist rahul bajaj city attend ...  label_2\n",
       "3599  victim’s father arrested charge covering tortu...  label_2\n",
       "3845  hide fight secure immediately” university said...  label_2\n",
       "\n",
       "[4992 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, all instances of one class are clumped together\n",
    "#Shuffling the rows of the dataframe to avoid discrepancies in future\n",
    "from sklearn.utils import shuffle\n",
    "trainDF = shuffle(trainDF)\n",
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Times_January.txt',encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(\"XX\") \n",
    "    texts.append(\" \".join(content[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe using above texts and labels\n",
    "import pandas\n",
    "testDF = pandas.DataFrame()\n",
    "testDF['text'] = texts\n",
    "testDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tripathy to debut in Sandalwood () Pitobash Tr...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>isn’t respect or fanfare: Yash on the fan who ...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>on ‘noisy’ bikes beat up 2 policemen | Jan 12,...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ali Khan soaking in the sun will make you beam...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rescues stranded tourists from north Sikkim in...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13854</th>\n",
       "      <td>that propelled BJP to power in 2014 has dissip...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13855</th>\n",
       "      <td>Delhi: Accused Mehtab arrested day after he ki...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13856</th>\n",
       "      <td>Nath may have been framed in rape case, observ...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13857</th>\n",
       "      <td>‘Can’t remove CM over pending cases’ | Jan 9, ...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13858</th>\n",
       "      <td></td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13859 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text label\n",
       "0      Tripathy to debut in Sandalwood () Pitobash Tr...    XX\n",
       "1      isn’t respect or fanfare: Yash on the fan who ...    XX\n",
       "2      on ‘noisy’ bikes beat up 2 policemen | Jan 12,...    XX\n",
       "3      Ali Khan soaking in the sun will make you beam...    XX\n",
       "4      rescues stranded tourists from north Sikkim in...    XX\n",
       "...                                                  ...   ...\n",
       "13854  that propelled BJP to power in 2014 has dissip...    XX\n",
       "13855  Delhi: Accused Mehtab arrested day after he ki...    XX\n",
       "13856  Nath may have been framed in rape case, observ...    XX\n",
       "13857  ‘Can’t remove CM over pending cases’ | Jan 9, ...    XX\n",
       "13858                                                       XX\n",
       "\n",
       "[13859 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>sacrilege therefore desecration unknown also s...</td>\n",
       "      <td>label_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>order receipt sent prime minister narendra mod...</td>\n",
       "      <td>label_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>demandedthe times india j86jcurrent 1956 proqu...</td>\n",
       "      <td>label_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>four monkeys found dead different parts udupi ...</td>\n",
       "      <td>label_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>celebration christian family attacked hindu ra...</td>\n",
       "      <td>label_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Yadav to launch poll on twitter LUCKNOW: Twitt...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>Gandhi to hold Kisan Abhar rally in Madhya Pra...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>man dies of injuries in car accident | Jan 11,...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>man sets self afire after tiff with wife, dies...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Plateau banks on tankers, bore wells PANAJI: F...</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5792 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    label\n",
       "414   sacrilege therefore desecration unknown also s...  label_1\n",
       "2038  order receipt sent prime minister narendra mod...  label_2\n",
       "605   demandedthe times india j86jcurrent 1956 proqu...  label_1\n",
       "4240  four monkeys found dead different parts udupi ...  label_2\n",
       "1804  celebration christian family attacked hindu ra...  label_1\n",
       "...                                                 ...      ...\n",
       "795   Yadav to launch poll on twitter LUCKNOW: Twitt...       XX\n",
       "796   Gandhi to hold Kisan Abhar rally in Madhya Pra...       XX\n",
       "797   man dies of injuries in car accident | Jan 11,...       XX\n",
       "798   man sets self afire after tiff with wife, dies...       XX\n",
       "799   Plateau banks on tankers, bore wells PANAJI: F...       XX\n",
       "\n",
       "[5792 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF1= testDF.iloc[0:800]\n",
    "result = trainDF.append(testDF1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = result\n",
    "#Splitting the dataset into training(80%) and validation(20%) sets  \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, train_y=trainDF['text'].iloc[0:4992], trainDF['label'].iloc[0:4992]\n",
    "valid_x, valid_y=trainDF['text'].iloc[4992:], trainDF['label'].iloc[4992:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      XX\n",
       "1      XX\n",
       "2      XX\n",
       "3      XX\n",
       "4      XX\n",
       "       ..\n",
       "795    XX\n",
       "796    XX\n",
       "797    XX\n",
       "798    XX\n",
       "799    XX\n",
       "Name: label, Length: 800, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414     label_1\n",
       "2038    label_2\n",
       "605     label_1\n",
       "4240    label_2\n",
       "1804    label_1\n",
       "         ...   \n",
       "4226    label_2\n",
       "237     label_1\n",
       "2109    label_2\n",
       "3599    label_2\n",
       "3845    label_2\n",
       "Name: label, Length: 4992, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding the class-labels or the target variable \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step is the feature engineering step. \n",
    "#Raw text data will be transformed into feature vectors and new features will be created using the existing dataset.\n",
    "\n",
    "#count vectorizer, tf-idf, word embeddings, topic models, part-of-speech tagging,\n",
    "\n",
    "#Creating count vectorizer object and using it to transform data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANKAN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:501: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=3000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a pre-trained word embedding to create a mapping of token and their respective embeddings\n",
    "import numpy\n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('C:/Users/ANKAN/Downloads/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec',encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "\n",
    "#Creatng a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "#Converting text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=50)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering - introducing new features that are expected to be useful\n",
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ANKAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>pron_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>noise &amp; gridlock capital of UP LUCKNOW: The ef...</td>\n",
       "      <td>XX</td>\n",
       "      <td>2221</td>\n",
       "      <td>365</td>\n",
       "      <td>6.068306</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>14</td>\n",
       "      <td>158</td>\n",
       "      <td>57</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>mothers to get Rs 10,000 for girl child | Jan ...</td>\n",
       "      <td>XX</td>\n",
       "      <td>982</td>\n",
       "      <td>165</td>\n",
       "      <td>5.915663</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>one of the oldest leopards dies at Van Vihar |...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1539</td>\n",
       "      <td>285</td>\n",
       "      <td>5.381119</td>\n",
       "      <td>35</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>89</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>members wrangle over quantum of punishment for...</td>\n",
       "      <td>XX</td>\n",
       "      <td>4557</td>\n",
       "      <td>794</td>\n",
       "      <td>5.732075</td>\n",
       "      <td>105</td>\n",
       "      <td>133</td>\n",
       "      <td>21</td>\n",
       "      <td>293</td>\n",
       "      <td>170</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>son starts charity | Jan 11, 2019, 08:35 IST S...</td>\n",
       "      <td>XX</td>\n",
       "      <td>636</td>\n",
       "      <td>102</td>\n",
       "      <td>6.174757</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Yadav to launch poll on twitter LUCKNOW: Twitt...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1095</td>\n",
       "      <td>173</td>\n",
       "      <td>6.293103</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>Gandhi to hold Kisan Abhar rally in Madhya Pra...</td>\n",
       "      <td>XX</td>\n",
       "      <td>2164</td>\n",
       "      <td>369</td>\n",
       "      <td>5.848649</td>\n",
       "      <td>37</td>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>160</td>\n",
       "      <td>65</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>man dies of injuries in car accident | Jan 11,...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1126</td>\n",
       "      <td>189</td>\n",
       "      <td>5.926316</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>man sets self afire after tiff with wife, dies...</td>\n",
       "      <td>XX</td>\n",
       "      <td>787</td>\n",
       "      <td>143</td>\n",
       "      <td>5.465278</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Plateau banks on tankers, bore wells PANAJI: F...</td>\n",
       "      <td>XX</td>\n",
       "      <td>3289</td>\n",
       "      <td>586</td>\n",
       "      <td>5.603066</td>\n",
       "      <td>59</td>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "      <td>169</td>\n",
       "      <td>112</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text label  char_count  \\\n",
       "790  noise & gridlock capital of UP LUCKNOW: The ef...    XX        2221   \n",
       "791  mothers to get Rs 10,000 for girl child | Jan ...    XX         982   \n",
       "792  one of the oldest leopards dies at Van Vihar |...    XX        1539   \n",
       "793  members wrangle over quantum of punishment for...    XX        4557   \n",
       "794  son starts charity | Jan 11, 2019, 08:35 IST S...    XX         636   \n",
       "795  Yadav to launch poll on twitter LUCKNOW: Twitt...    XX        1095   \n",
       "796  Gandhi to hold Kisan Abhar rally in Madhya Pra...    XX        2164   \n",
       "797  man dies of injuries in car accident | Jan 11,...    XX        1126   \n",
       "798  man sets self afire after tiff with wife, dies...    XX         787   \n",
       "799  Plateau banks on tankers, bore wells PANAJI: F...    XX        3289   \n",
       "\n",
       "     word_count  word_density  punctuation_count  title_word_count  \\\n",
       "790         365      6.068306                 55                45   \n",
       "791         165      5.915663                 19                12   \n",
       "792         285      5.381119                 35                48   \n",
       "793         794      5.732075                105               133   \n",
       "794         102      6.174757                 15                20   \n",
       "795         173      6.293103                 20                34   \n",
       "796         369      5.848649                 37                72   \n",
       "797         189      5.926316                 27                29   \n",
       "798         143      5.465278                 23                22   \n",
       "799         586      5.603066                 59                66   \n",
       "\n",
       "     upper_case_word_count  noun_count  verb_count  adj_count  adv_count  \\\n",
       "790                     14         158          57         21          8   \n",
       "791                      2          60          19         15          1   \n",
       "792                      6          89          45         32         12   \n",
       "793                     21         293         170         45         32   \n",
       "794                      3          43          13          6          4   \n",
       "795                      4          66          29         15          5   \n",
       "796                      6         160          65         13         18   \n",
       "797                      5          67          38          9          4   \n",
       "798                      2          47          27          5          4   \n",
       "799                     10         169         112         35         31   \n",
       "\n",
       "     pron_count  \n",
       "790           5  \n",
       "791           4  \n",
       "792          16  \n",
       "793          32  \n",
       "794           8  \n",
       "795          10  \n",
       "796           3  \n",
       "797           8  \n",
       "798          15  \n",
       "799          31  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Count Vectors:  0.995\n",
      "10 fold Cross-validated score:  0.9775659318637275 \n",
      "\n",
      "Naive Bayes WordLevel TF-IDF:  0.975\n",
      "10 fold Cross-validated score:  0.9829743486973946 \n",
      "\n",
      "Naive Bayes N-Gram Vectors:  1.0\n",
      "10 fold Cross-validated score:  0.8946336673346693 \n",
      "\n",
      "Naive Bayes CharLevel Vectors:  1.0\n",
      "10 fold Cross-validated score:  0.9527234468937875 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"Naive Bayes Count Vectors: \", accuracy)\n",
    "scores = cross_val_score(naive_bayes.MultinomialNB(), xtrain_count, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Naive Bayes WordLevel TF-IDF: \", accuracy)\n",
    "scores = cross_val_score(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"Naive Bayes N-Gram Vectors: \", accuracy)\n",
    "scores = cross_val_score(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"Naive Bayes CharLevel Vectors: \", accuracy)\n",
    "scores = cross_val_score(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, Count Vectors:  0.02875\n",
      "10 fold Cross-validated score:  0.9855779559118236 \n",
      "\n",
      "Logistic Regression, WordLevel TF-IDF:  0.0725\n",
      "10 fold Cross-validated score:  0.9899851703406813 \n",
      "\n",
      "Logistic Regression, N-Gram Vectors:  0.94375\n",
      "10 fold Cross-validated score:  0.9535266533066131 \n",
      "\n",
      "Logistic Regression, CharLevel Vectors:  0.51375\n",
      "10 fold Cross-validated score:  0.982972745490982 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(max_iter=4000), xtrain_count, train_y, xvalid_count)\n",
    "print (\"Logistic Regression, Count Vectors: \", accuracy)\n",
    "scores = cross_val_score(linear_model.LogisticRegression(max_iter=4000), xtrain_count, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Logistic Regression, WordLevel TF-IDF: \", accuracy)\n",
    "scores = cross_val_score(linear_model.LogisticRegression(max_iter=4000), xtrain_tfidf, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"Logistic Regression, N-Gram Vectors: \", accuracy)\n",
    "scores = cross_val_score(linear_model.LogisticRegression(max_iter=4000), xtrain_tfidf_ngram, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"Logistic Regression, CharLevel Vectors: \", accuracy)\n",
    "scores = cross_val_score(linear_model.LogisticRegression(max_iter=4000), xtrain_tfidf_ngram_chars, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.86875\n",
      "10 fold Cross-validated score:  0.9669478957915831 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)\n",
    "scores = cross_val_score(svm.SVC(), xtrain_tfidf_ngram, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests, Count Vectors:  0.18\n",
      "10 fold Cross-validated score:  0.9733583166332664 \n",
      "\n",
      "Random Forests, WordLevel TF-IDF:  0.07375\n",
      "10 fold Cross-validated score:  0.9839747494989981 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forests on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"Random Forests, Count Vectors: \", accuracy)\n",
    "scores = cross_val_score(ensemble.RandomForestClassifier(), xtrain_count, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Random Forests, WordLevel TF-IDF: \", accuracy)\n",
    "scores = cross_val_score(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.08625\n",
      "10 fold Cross-validated score:  0.987182765531062 \n",
      "\n",
      "Xgb, WordLevel TF-IDF:  0.03625\n",
      "10 fold Cross-validated score:  0.9851783567134268 \n",
      "\n",
      "Xgb, CharLevel Vectors:  0.1075\n",
      "10 fold Cross-validated score:  0.9837743486973949 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "scores = cross_val_score(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "scores = cross_val_score(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)\n",
    "scores = cross_val_score(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, cv=10)\n",
    "print (\"10 fold Cross-validated score: \", np.mean(scores), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rmf = RandomForestClassifier()\n",
    "rmf.fit(xtrain_tfidf, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=rmf.predict(xvalid_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-f2ab29ad97b7>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testDF1['predicted']=y_pred\n"
     ]
    }
   ],
   "source": [
    "testDF1['predicted']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tripathy to debut in Sandalwood () Pitobash Tr...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>isn’t respect or fanfare: Yash on the fan who ...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>on ‘noisy’ bikes beat up 2 policemen | Jan 12,...</td>\n",
       "      <td>XX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ali Khan soaking in the sun will make you beam...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rescues stranded tourists from north Sikkim in...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Yadav to launch poll on twitter LUCKNOW: Twitt...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>Gandhi to hold Kisan Abhar rally in Madhya Pra...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>man dies of injuries in car accident | Jan 11,...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>man sets self afire after tiff with wife, dies...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Plateau banks on tankers, bore wells PANAJI: F...</td>\n",
       "      <td>XX</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text label  predicted\n",
       "0    Tripathy to debut in Sandalwood () Pitobash Tr...    XX          1\n",
       "1    isn’t respect or fanfare: Yash on the fan who ...    XX          1\n",
       "2    on ‘noisy’ bikes beat up 2 policemen | Jan 12,...    XX          0\n",
       "3    Ali Khan soaking in the sun will make you beam...    XX          1\n",
       "4    rescues stranded tourists from north Sikkim in...    XX          1\n",
       "..                                                 ...   ...        ...\n",
       "795  Yadav to launch poll on twitter LUCKNOW: Twitt...    XX          1\n",
       "796  Gandhi to hold Kisan Abhar rally in Madhya Pra...    XX          1\n",
       "797  man dies of injuries in car accident | Jan 11,...    XX          1\n",
       "798  man sets self afire after tiff with wife, dies...    XX          1\n",
       "799  Plateau banks on tankers, bore wells PANAJI: F...    XX          1\n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF1.to_csv('predicted_stage_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
